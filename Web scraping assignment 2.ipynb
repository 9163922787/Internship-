{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eb961a",
   "metadata": {},
   "source": [
    "# Q1. Write a python program to scrape data for \"Data Analyst\" Job position in \"Bangalore\" location. You have to scrape the job-title,job-location,company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "41d302b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Rupeek Fintech Pvt Ltd</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Flipkart Analytics</td>\n",
       "      <td>Bangalore/Bengaluru(Bellandur)</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Advanced Computer Software</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Data Analyst - Database Design/Mining</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AugmatrixGo</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urgent Openings For Data Analyst / Business An...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Professional 1 Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business Data Analyst(BigId) - Capco - Bangalore</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Capco Technologies Pvt Ltd</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Kochi/Cochin, Cannanore/Kannur, Nagpur, Calicu...</td>\n",
       "      <td>CINDREBAY SCHOOL OF FASHION &amp; INTERIOR DESIGN</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IDC CENTRE FOR CONSULTANCY AND RESEARCH PRIVAT...</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                                       Data Analyst   \n",
       "1                  Data Analyst - Flipkart Analytics   \n",
       "2                                Senior Data Analyst   \n",
       "3     Business Data Analyst - Database Design/Mining   \n",
       "4  Urgent Openings For Data Analyst / Business An...   \n",
       "5                        Professional 1 Data Analyst   \n",
       "6   Business Data Analyst(BigId) - Capco - Bangalore   \n",
       "7                                Senior Data Analyst   \n",
       "8                                       Data Analyst   \n",
       "9                                       Data Analyst   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                     Bangalore/Bengaluru(Bellandur)   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8  Kochi/Cochin, Cannanore/Kannur, Nagpur, Calicu...   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                             Company Experience  \n",
       "0                             Rupeek Fintech Pvt Ltd    0-2 Yrs  \n",
       "1                                           Flipkart    0-3 Yrs  \n",
       "2                         Advanced Computer Software    3-5 Yrs  \n",
       "3                                        AugmatrixGo    2-5 Yrs  \n",
       "4                                           Flipkart    1-6 Yrs  \n",
       "5                                     DXC Technology    3-8 Yrs  \n",
       "6                         Capco Technologies Pvt Ltd    3-8 Yrs  \n",
       "7                                            Red Hat    2-5 Yrs  \n",
       "8      CINDREBAY SCHOOL OF FASHION & INTERIOR DESIGN    1-4 Yrs  \n",
       "9  IDC CENTRE FOR CONSULTANCY AND RESEARCH PRIVAT...    3-7 Yrs  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "# get the webpage www.naukri.com\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(5)\n",
    "driver.switch_to.window(driver.window_handles[0]) # switch to first window\n",
    "time.sleep(2)\n",
    "#finding elements for job search bar\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_loc= driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "# creating xpath for the search with set elements\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "# click the search button\n",
    "search_btn.click()\n",
    "time.sleep(5)\n",
    "#scraping the job_titles for data analyst in Bangalore(First 10)\n",
    "titles_tags = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')[:10]\n",
    "\n",
    "#storing in list without extra tags\n",
    "job_titles=[]\n",
    "for i in titles_tags:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "#scraping the company names using xpath\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[:10]\n",
    "#storing in company_names list without extra tags\n",
    "company_names=[]\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "\n",
    "#getting extra data using span,class name =\"ellipsis fleft fs12 lh16\". so we use parent tag li\n",
    "#to avoid extra data i.e.(WHF During Covid)  we will consider first span element\n",
    "#scraping location data \n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[:10]\n",
    "#storing in location list without extra tags\n",
    "location_list=[]\n",
    "for i in location_tags:\n",
    "    location_list.append(i.text)\n",
    "location_list\n",
    "# extract the years of experience needed (first 10 elements)\n",
    "job_exp = driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')[:10]\n",
    "#stroring data in list\n",
    "experience_list = []\n",
    "for i in job_exp:\n",
    "    experience_list.append(i.text)\n",
    "#Checking the equal length of list\n",
    "print(len(job_titles),len(company_names),len(location_list),len(experience_list))\n",
    "\n",
    "#converting into dataframe\n",
    "df_analyst=pd.DataFrame({\"Job Title\":job_titles,\"Location\":location_list,\"Company\":company_names,\"Experience\":experience_list})\n",
    "driver.close()\n",
    "df_analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e43969",
   "metadata": {},
   "source": [
    "#  Q.2 Write a python program to scrape data for \"Data Scientist\" Job Position in \"Bangalore\" location.You have to scrape the job-title, job-location, company_name. You have to  scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6b3143e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Process Innovation Analyst - APAC/Data Scienti...</td>\n",
       "      <td>Bangalore/Bengaluru, Hyderabad</td>\n",
       "      <td>Bayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist: Artificial Intelligence</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Hitachi Ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist - Insights</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Autodesk India Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist - Computer Vision</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GE Transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist | Fortune 500 Supermarke...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>TALENT500 TECH (INDIA) PRIVATE LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist /Senior Data Scientist</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Dun &amp; Bradstreet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sr Data Scientist / Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>intelligent industrial internet systems pvt ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Siemens Limited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Process Innovation Analyst - APAC/Data Scienti...   \n",
       "1                 Data Scientist: Advanced Analytics   \n",
       "2            Data Scientist: Artificial Intelligence   \n",
       "3                              Senior Data Scientist   \n",
       "4                   Senior Data Scientist - Insights   \n",
       "5            Senior Data Scientist - Computer Vision   \n",
       "6  Senior Data Scientist | Fortune 500 Supermarke...   \n",
       "7              Data Scientist /Senior Data Scientist   \n",
       "8                 Sr Data Scientist / Data Scientist   \n",
       "9                              Senior Data Scientist   \n",
       "\n",
       "                                  Location  \\\n",
       "0           Bangalore/Bengaluru, Hyderabad   \n",
       "1                      Bangalore/Bengaluru   \n",
       "2                      Bangalore/Bengaluru   \n",
       "3                      Bangalore/Bengaluru   \n",
       "4                      Bangalore/Bengaluru   \n",
       "5                      Bangalore/Bengaluru   \n",
       "6                      Bangalore/Bengaluru   \n",
       "7             Chennai, Bangalore/Bengaluru   \n",
       "8  Bangalore/Bengaluru, Mumbai (All Areas)   \n",
       "9                      Bangalore/Bengaluru   \n",
       "\n",
       "                                            Company  \n",
       "0                                             Bayer  \n",
       "1                            IBM India Pvt. Limited  \n",
       "2                            IBM India Pvt. Limited  \n",
       "3                                      Hitachi Ltd.  \n",
       "4                            Autodesk India Pvt Ltd  \n",
       "5                                 GE Transportation  \n",
       "6            TALENT500 TECH (INDIA) PRIVATE LIMITED  \n",
       "7                                  Dun & Bradstreet  \n",
       "8  intelligent industrial internet systems pvt ltd.  \n",
       "9                                   Siemens Limited  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "# get the webpage www.naukri.com\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(5)\n",
    "\n",
    "driver.switch_to.window(driver.window_handles[0]) # switch to first window\n",
    "time.sleep(2)\n",
    "#code to write Data Scientist in skill,designation,companies\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "# code to write Bangalore in location box\n",
    "search_loc = driver.find_element_by_xpath('//input[@name=\"location\"]')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "# code to click on the search button\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"btn\"]')\n",
    "search_btn.click()\n",
    "\n",
    "time.sleep(5)\n",
    "#scraping the job_titles for data Scientist in Bangalore(First 10)\n",
    "titles_tags = driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')[:10]\n",
    "#storing in list without extra tags\n",
    "job_titles=[]\n",
    "for i in titles_tags:\n",
    "    job_titles.append(i.text)\n",
    "job_titles\n",
    "#scraping the company names using xpath\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[:10]\n",
    "#storing in company_names list without extra tags\n",
    "company_names=[]\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "company_names\n",
    "#getting extra data using span,class name =\"ellipsis fleft fs12 lh16\". so we use parent tag li\n",
    "#to avoid extra data i.e.(WHF During Covid)  we will consider first span element\n",
    "#scraping location data \n",
    "\n",
    "#scraping location data \n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[:10]\n",
    "#location_tags\n",
    "#storing in location list without extra tags\n",
    "location_list=[]\n",
    "for i in location_tags:\n",
    "    location_list.append(i.text)\n",
    "location_list\n",
    "#Checking the equal length of list\n",
    "print(len(job_titles),len(company_names),len(location_list))\n",
    "#converting into dataframe\n",
    "df_science=pd.DataFrame({\"Job Title\":job_titles,\"Location\":location_list,\"Company\":company_names})\n",
    "driver.close()\n",
    "df_science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6e8b2",
   "metadata": {},
   "source": [
    "# Q.3:You have to use the location and salary filter, you have to scrape data for \"Data Scientist\" designation for first 10 job results,    You have to scrape the job-title,job-location,company name, experience required,    The location filter to be used is \"Delhi/NCR\" The salary filter to be used is \"3-6\" lakhs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5415b5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist III-2</td>\n",
       "      <td>Concentrix Daksh Services</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Job Opportunity || Data Scientist || HCL Techn...</td>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ThinkBumblebee Analytics Pvt. Ltd.</td>\n",
       "      <td>Pune, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>MoMagic Technologies Pvt. Ltd.</td>\n",
       "      <td>Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>MoMagic Technologies Pvt. Ltd.</td>\n",
       "      <td>Noida(Sector-126 Noida)</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Urgent Hiring || Data Scientist || Delhi</td>\n",
       "      <td>Shriram Automall India Limited</td>\n",
       "      <td>Noida(Sector-126 Noida)</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Think i</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Academic Counsellor - Data Scientist</td>\n",
       "      <td>GreatLearning</td>\n",
       "      <td>Kochi/Cochin, Kolkata, Hyderabad/Secunderabad,...</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0                               Data Scientist III-2   \n",
       "1  Job Opportunity || Data Scientist || HCL Techn...   \n",
       "2                                     Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4                                     Data Scientist   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7           Urgent Hiring || Data Scientist || Delhi   \n",
       "8                                     Data Scientist   \n",
       "9               Academic Counsellor - Data Scientist   \n",
       "\n",
       "                                          Company  \\\n",
       "0                       Concentrix Daksh Services   \n",
       "1                                HCL Technologies   \n",
       "2              ThinkBumblebee Analytics Pvt. Ltd.   \n",
       "3  Optum Global Solutions (India) Private Limited   \n",
       "4                               Fractal Analytics   \n",
       "5                  MoMagic Technologies Pvt. Ltd.   \n",
       "6                  MoMagic Technologies Pvt. Ltd.   \n",
       "7                  Shriram Automall India Limited   \n",
       "8                                         Think i   \n",
       "9                                   GreatLearning   \n",
       "\n",
       "                                            Location Experience  \n",
       "0                                   Gurgaon/Gurugram    3-8 Yrs  \n",
       "1                                        Delhi / NCR    4-7 Yrs  \n",
       "2             Pune, Bangalore/Bengaluru, Delhi / NCR    2-6 Yrs  \n",
       "3                                 (WFH during Covid)    2-6 Yrs  \n",
       "4                            Noida, Gurgaon/Gurugram    3-7 Yrs  \n",
       "5      Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru    4-6 Yrs  \n",
       "6                            Noida(Sector-126 Noida)    4-6 Yrs  \n",
       "7                            Noida(Sector-126 Noida)    2-7 Yrs  \n",
       "8                                        Delhi / NCR    0-2 Yrs  \n",
       "9  Kochi/Cochin, Kolkata, Hyderabad/Secunderabad,...    1-4 Yrs  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "# get the webpage www.naukri.com\n",
    "driver.get('https://www.naukri.com/')\n",
    "time.sleep(5)\n",
    "\n",
    "driver.switch_to.window(driver.window_handles[0]) # switch to first window\n",
    "time.sleep(2)\n",
    "#code to write Data Scientist in skill,designation,companies\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "time.sleep(5)\n",
    "# code to click on the search button\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"btn\"]')\n",
    "search_btn.click()\n",
    "time.sleep(10)\n",
    "\n",
    "# code for closing the chat box appears \n",
    "try:\n",
    "    chat_close=driver.find_element_by_xpath(\"//div[@class='crossIcon chatBot chatBot-ic-cross']\")\n",
    "    chat_close.click()# close the chat box\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "# xpath for location Delhi/NCR\n",
    "loc_filter=driver.find_element_by_xpath('//div[@class=\"mt-8 chckBoxCont\"]/label[@for=\"chk-Delhi / NCR-cityTypeGid-\"]/i')\n",
    "loc_filter.click()# select location filter Delhi/NCR\n",
    "time.sleep(5)\n",
    "#xpath for salary 3-6 lakhs\n",
    "salary_filter=driver.find_element_by_xpath('//div[@class=\"mt-8 chckBoxCont\"]/label[@for=\"chk-3-6 Lakhs-ctcFilter-\"]/i')\n",
    "salary_filter.click()#applying filter salary 3-6 lacs\n",
    "time.sleep(5)\n",
    "\n",
    "job_titles=[]\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[:10]\n",
    "for i in title_tags:\n",
    "    job_titles.append(i.text)\n",
    "    \n",
    "company_names=[]\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[:10]\n",
    "for i in company_tags:\n",
    "    company_names.append(i.text)\n",
    "        \n",
    "experience_list=[]\n",
    "exp_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")[:10]\n",
    "for i in exp_tags:\n",
    "    experience_list.append(i.text)\n",
    "        \n",
    "locations_list=[]\n",
    "loc_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")[:10]\n",
    "for i in loc_tags:\n",
    "    locations_list.append(i.text)\n",
    "#converting into dataframe\n",
    "df=pd.DataFrame({\"Job Title\":job_titles,\"Company\":company_names,\"Location\":locations_list,\"Experience\":experience_list})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a5467",
   "metadata": {},
   "source": [
    "Q.4 Scrape data of first 100 sunglasses listing on flipkart.com. You have to scrape four attribute:\n",
    "    1.Brand\n",
    "    2.Product Description\n",
    "    3.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "38d5f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Retro Square Sunglasse...</td>\n",
       "      <td>₹349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VILLAIN</td>\n",
       "      <td>Others Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Round Sungl...</td>\n",
       "      <td>₹276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>elegante</td>\n",
       "      <td>UV Protection Oval Sunglasses (Free Size)</td>\n",
       "      <td>₹359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection, Gradient Wayfarer Sunglasses (53)</td>\n",
       "      <td>₹296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored, Night Vision, Riding ...</td>\n",
       "      <td>₹364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Mirrored, Riding Glasses, Others Sports Sungla...</td>\n",
       "      <td>₹233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection, Gradient Over-sized,...</td>\n",
       "      <td>₹1,234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                        Description   Price\n",
       "0   ROZZETTA CRAFT  UV Protection, Gradient Retro Square Sunglasse...    ₹349\n",
       "1          VILLAIN         Others Retro Square Sunglasses (Free Size)    ₹549\n",
       "2         Fastrack   UV Protection Rectangular Sunglasses (Free Size)    ₹639\n",
       "3           SUNBEE  UV Protection, Polarized, Mirrored Round Sungl...    ₹276\n",
       "4        Elligator                UV Protection Round Sunglasses (54)    ₹248\n",
       "..             ...                                                ...     ...\n",
       "95        elegante          UV Protection Oval Sunglasses (Free Size)    ₹359\n",
       "96          GANSTA   UV Protection, Gradient Wayfarer Sunglasses (53)    ₹296\n",
       "97           NuVew  UV Protection, Mirrored, Night Vision, Riding ...    ₹364\n",
       "98    Singco India  Mirrored, Riding Glasses, Others Sports Sungla...    ₹233\n",
       "99       ROYAL SON  Polarized, UV Protection, Gradient Over-sized,...  ₹1,234\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "# get the webpage www.flipcart.com\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(10)\n",
    "#closing the popup\n",
    "close_popup = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "close_popup.click()\n",
    "# write sunglasses in the search input box\n",
    "search_input = driver.find_element_by_xpath('//input[@class=\"_3704LK\"]')\n",
    "search_input.send_keys(\"Sunglasses\")\n",
    "# click on the search button\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "search_btn.click()\n",
    "time.sleep(5)\n",
    "\n",
    "url_list=[]\n",
    "url=driver.find_elements_by_xpath('//nav[@class=\"yFHi8N\"]/a')\n",
    "\n",
    "for i in url[:3]:\n",
    "    url_list.append(i.get_attribute('href'))\n",
    "time.sleep(20)\n",
    "\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "\n",
    "for i in url_list:\n",
    "    driver.get(i)\n",
    "    # scraping the sunglasses brand details\n",
    "    glass_brand = driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')# xpath for brand\n",
    "    for i in glass_brand:\n",
    "        brand.append(i.text) # saving in the list \n",
    "\n",
    "    # scraping the sunglasses the description\n",
    "    glass_description = driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]') # xpath for description\n",
    "    for i in glass_description:\n",
    "        try:\n",
    "            description.append(i.text)# saving in the list\n",
    "        except:\n",
    "            description.appendend(\"There is no description\")\n",
    "\n",
    "    # scraping the price of the sunglasses\n",
    "    glass_price = driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')# xpath for price\n",
    "    for i in glass_price:\n",
    "        price.append(i.text)# saving in the list\n",
    "time.sleep(5)\n",
    "\n",
    "#checking the length of data\n",
    "print( len(brand), len(description), len(price))\n",
    "#driver.close()\n",
    "# saving the lists as dataframe\n",
    "df=pd.DataFrame({\"Brand\":brand[:100],\"Description\":description[:100],\"Price\":price[:100]})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f2032",
   "metadata": {},
   "source": [
    "Q.5 Scrape 100 reviews data from flipkart.com for iphone11 phone. url given in Question\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bcb9d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 110 110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ph Rating</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This is my first ever iPhone.\\nAnd I truly don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>The ultimate performance\\nCamera is superb\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>I use a Note10+ and have been using both iOS a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>The phone is completely good\\nAs far as camera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Decent product</td>\n",
       "      <td>Everything u ll like it when u use this iPhone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ph Rating            Summary  \\\n",
       "0          5          Brilliant   \n",
       "1          5     Simply awesome   \n",
       "2          5   Perfect product!   \n",
       "3          5  Worth every penny   \n",
       "4          5          Fabulous!   \n",
       "..       ...                ...   \n",
       "95         5             Super!   \n",
       "96         3          Just wow!   \n",
       "97         5  Terrific purchase   \n",
       "98         5            Awesome   \n",
       "99         5     Decent product   \n",
       "\n",
       "                                          Full Review  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Amazing phone with great cameras and better ba...  \n",
       "3   Previously I was using one plus 3t it was a gr...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  This is my first ever iPhone.\\nAnd I truly don...  \n",
       "96  The ultimate performance\\nCamera is superb\\nTh...  \n",
       "97  I use a Note10+ and have been using both iOS a...  \n",
       "98  The phone is completely good\\nAs far as camera...  \n",
       "99  Everything u ll like it when u use this iPhone...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\"\n",
    "driver.get(url)\n",
    "time.sleep(10)\n",
    "\n",
    "all_reviews=driver.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']\") #finding the veiw all review link\n",
    "all_reviews.click()# clicking view all\n",
    "time.sleep(10)\n",
    "\n",
    "# there are 10 pages (Each with 10 reviews), we need to store the urls in list\n",
    "url = driver.find_elements_by_xpath('//nav[@class=\"yFHi8N\"]//a') # using the paging navigation bar \n",
    "#for i in url:\n",
    "#    print(i.text)\n",
    "flipkart_urls=[]\n",
    "for i in url[:11]:\n",
    "    flipkart_urls.append(i.get_attribute('href')) # saving the urls in list \n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "rv_ratings=[]\n",
    "rv_summary=[]\n",
    "rv_full=[]\n",
    "for i in flipkart_urls:# looping through the urls\n",
    "    driver.get(i)#loading the contents of each urls in loop\n",
    "    time.sleep(5)\n",
    "    ratings = driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]') # xpath for rating \n",
    "    for j in ratings:\n",
    "        rv_ratings.append(j.text)# storing  10 rating in second loop\n",
    "    summary = driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]') # xpath for summary\n",
    "    for j in summary:# storing  10 summary in second loop\n",
    "        rv_summary.append(j.text)\n",
    "\n",
    "    full_review = driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')# xpath for full_review\n",
    "    for j in full_review:# storing  10 full review in second loop\n",
    "        rv_full.append(j.text)\n",
    "\n",
    "print(len(rv_ratings),len(rv_summary),len(rv_full))# checking the equality of list\n",
    "#creating the dataframe\n",
    "df=pd.DataFrame({\"Ph Rating\":rv_ratings[:100],\"Summary\":rv_summary[:100],\"Full Review\":rv_full[:100]})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980b60c",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "72a97a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 119\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sneaker Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TWIN TOES</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Puma Smash v2 SL one8 Sneakers For Men</td>\n",
       "      <td>₹3,119</td>\n",
       "      <td>22% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹479</td>\n",
       "      <td>63% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wlinzak</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹450</td>\n",
       "      <td>54% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SCATCHITE</td>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>₹398</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>MAPF1 A3ROCAT Mid Sneakers For Men</td>\n",
       "      <td>₹7,799</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>516 Trendy Star Perfect Sneakers For Men</td>\n",
       "      <td>₹297</td>\n",
       "      <td>33% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Jack Diamond</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹664</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Rzisbo</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹549</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Neeman's</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹2,399</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sneaker Brand                               Description   Price Discount\n",
       "0      TWIN TOES                          Sneakers For Men    ₹299  70% off\n",
       "1           PUMA    Puma Smash v2 SL one8 Sneakers For Men  ₹3,119  22% off\n",
       "2        Numenzo                          Sneakers For Men    ₹479  63% off\n",
       "3        Wlinzak                          Sneakers For Men    ₹450  54% off\n",
       "4      SCATCHITE                 Sneakers Sneakers For Men    ₹398  60% off\n",
       "..           ...                                       ...     ...      ...\n",
       "95          PUMA        MAPF1 A3ROCAT Mid Sneakers For Men  ₹7,799  62% off\n",
       "96        Chevit  516 Trendy Star Perfect Sneakers For Men    ₹297  33% off\n",
       "97  Jack Diamond                          Sneakers For Men    ₹664  45% off\n",
       "98        Rzisbo                          Sneakers For Men    ₹549  20% off\n",
       "99      Neeman's                          Sneakers For Men  ₹2,399  55% off\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "driver.get(\"https://flipkart.com\") #opening the url\n",
    "time.sleep(10)\n",
    "try:\n",
    "    #closing the popup\n",
    "    close_popup = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "    close_popup.click()\n",
    "except NoSuchElementException:\n",
    "    pass\n",
    "\n",
    "time.sleep(2)\n",
    "search_input=driver.find_element_by_xpath(\"//input[@class='_3704LK']\")# set sneakers in input box\n",
    "search_input.send_keys(\"sneakers\")\n",
    "\n",
    "search_button = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")# click the search button\n",
    "search_button.click()\n",
    "time.sleep(5)\n",
    "url_list=[]\n",
    "page=driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']//a\") # finding the paging numbers\n",
    "for i in (page[:3]):\n",
    "    url_list.append(i.get_attribute('href')) # saving the href links for firat three pages\n",
    "time.sleep(10)\n",
    "snkr_brand=[]\n",
    "snkr_description=[]\n",
    "snkr_price=[]\n",
    "snkr_discount=[]\n",
    "for i in url_list: # loop through the there pages link \n",
    "    driver.get(i) # opening three pages \n",
    "    brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")# xpath for brand \n",
    "    for i in brand:\n",
    "        snkr_brand.append(i.text) # scraping the brand name and append  in list\n",
    "    description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\") #xpath for description\n",
    "    for i in description:\n",
    "        snkr_description.append(i.text)# scraping the description and append in the list\n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[2]/div/div[1]\") # xpath for price\n",
    "    for i in price:\n",
    "        snkr_price.append(i.text) # scraping the price and append in the list\n",
    "    discount=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[2]/div/div[3]\") # xpath for discount\n",
    "    for i in discount:\n",
    "        snkr_discount.append(i.text) # scraping the discount and appned in the list\n",
    "time.sleep(5)\n",
    "# checking the lenght of each list\n",
    "print(len(snkr_brand),len(snkr_description),len(snkr_price),len(snkr_discount))\n",
    "# creating dataframe\n",
    "df=pd.DataFrame({'Sneaker Brand':snkr_brand[:100],'Description':snkr_description[:100],'Price':snkr_price[:100],'Discount':snkr_discount[:100]})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6770373",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f1974908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shoe Brand</th>\n",
       "      <th>Small Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>AIR ZOOM PEGASUS Running Shoes</td>\n",
       "      <td>Rs. 7996Rs. 9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Unisex Scuderia Ferrari Shoes</td>\n",
       "      <td>Rs. 7999Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Span 4 Running Shoes</td>\n",
       "      <td>Rs. 7195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men AIR PRESTO Sneakers</td>\n",
       "      <td>Rs. 9196Rs. 11495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Jamming 2.0 Running Shoes</td>\n",
       "      <td>Rs. 10399Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Formal Slip-Ons</td>\n",
       "      <td>Rs. 8999Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROSSO BRUNELLO</td>\n",
       "      <td>Men Woven Design Leather Loafers</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Derbys</td>\n",
       "      <td>Rs. 7199Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Magnify Nitro Running</td>\n",
       "      <td>Rs. 8449Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Flat Boots</td>\n",
       "      <td>Rs. 11999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Shoe Brand                 Small Description               Price\n",
       "0              Nike    AIR ZOOM PEGASUS Running Shoes    Rs. 7996Rs. 9995\n",
       "1   PUMA Motorsport     Unisex Scuderia Ferrari Shoes    Rs. 7999Rs. 9999\n",
       "2              Nike     Men Zoom Span 4 Running Shoes            Rs. 7195\n",
       "3              Nike           Men AIR PRESTO Sneakers   Rs. 9196Rs. 11495\n",
       "4              Puma     Men Jamming 2.0 Running Shoes  Rs. 10399Rs. 12999\n",
       "..              ...                               ...                 ...\n",
       "95     Hush Puppies         Men Solid Formal Slip-Ons    Rs. 8999Rs. 9999\n",
       "96   ROSSO BRUNELLO  Men Woven Design Leather Loafers           Rs. 10999\n",
       "97     Hush Puppies   Men Solid Leather Formal Derbys    Rs. 7199Rs. 8999\n",
       "98             Puma         Men Magnify Nitro Running   Rs. 8449Rs. 12999\n",
       "99             Geox            Men Leather Flat Boots           Rs. 11999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.myntra.com/shoes\") #opening the url\n",
    "#applying price filter\n",
    "price_filter=driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]/label/div\")\n",
    "price_filter.click()\n",
    "time.sleep(5)\n",
    "#applying filter color black\n",
    "color_filter=driver.find_element_by_xpath('//li[@class=\"colour-listItem\"]/label/div')\n",
    "color_filter.click()\n",
    "time.sleep(5)\n",
    "\n",
    "shoe_brand=[]\n",
    "shoe_description=[]\n",
    "shoe_price=[]\n",
    "def scrap_shoes(driver):\n",
    "    brand=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h3\")\n",
    "    for i in brand:\n",
    "        shoe_brand.append(i.text)\n",
    "\n",
    "    description=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4[1]\")\n",
    "    for i in description:\n",
    "        shoe_description.append(i.text)\n",
    "\n",
    "    price=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/div/span[1]\")\n",
    "    for i in price:\n",
    "        shoe_price.append(i.text)\n",
    "\n",
    "scrap_shoes(driver)        \n",
    "\n",
    "while((len(shoe_brand) < 100) and (len(shoe_description)<100) and (len(shoe_price)<100)):\n",
    "    next_page=driver.find_element_by_xpath('//a[@rel=\"next\"]')\n",
    "    next_page.click()\n",
    "    time.sleep(5)    \n",
    "    page=driver.find_element_by_xpath('//li[@class=\"pagination-active\"]/a')\n",
    "    url=page.get_attribute('href')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    scrap_shoes(driver)\n",
    "\n",
    "#print(len(shoe_brand),len(shoe_description),len(shoe_price))\n",
    "df=pd.DataFrame({\"Shoe Brand\":shoe_brand,\"Small Description\":shoe_description,\"Price\":shoe_price})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f7756",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3addfe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS TUF Dash F15 (2021) 15.6-inch (39.62 cms)...</td>\n",
       "      <td></td>\n",
       "      <td>93,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASUS TUF Dash F15 (2021), 15.6-inch (39.62 cms...</td>\n",
       "      <td></td>\n",
       "      <td>95,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i7-1...</td>\n",
       "      <td></td>\n",
       "      <td>56,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 1...</td>\n",
       "      <td></td>\n",
       "      <td>1,09,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion (2021) Intel 11th Gen Core i7 14 i...</td>\n",
       "      <td></td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS TUF Gaming F17 (2021), 17.3-inch (43.94 c...</td>\n",
       "      <td></td>\n",
       "      <td>1,13,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Acer Nitro 5 AN515-57 Gaming Laptop | Intel Co...</td>\n",
       "      <td></td>\n",
       "      <td>85,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dell 14 (2021) i7-1195G7 2in1 Touch Screen Lap...</td>\n",
       "      <td></td>\n",
       "      <td>88,508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo Legion 5 11th Gen Intel Core i7 15.6\"(3...</td>\n",
       "      <td></td>\n",
       "      <td>1,11,030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td></td>\n",
       "      <td>85,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Laptop Title Rating     Price\n",
       "0  ASUS TUF Dash F15 (2021) 15.6-inch (39.62 cms)...           93,990\n",
       "1  ASUS TUF Dash F15 (2021), 15.6-inch (39.62 cms...           95,990\n",
       "2  Mi Notebook Horizon Edition 14 Intel Core i7-1...           56,990\n",
       "3  Lenovo Yoga 7i 11th Gen Intel Core i7-1165G7 1...         1,09,000\n",
       "4  HP Pavilion (2021) Intel 11th Gen Core i7 14 i...           84,990\n",
       "5  ASUS TUF Gaming F17 (2021), 17.3-inch (43.94 c...         1,13,990\n",
       "6  Acer Nitro 5 AN515-57 Gaming Laptop | Intel Co...           85,990\n",
       "7  Dell 14 (2021) i7-1195G7 2in1 Touch Screen Lap...           88,508\n",
       "8  Lenovo Legion 5 11th Gen Intel Core i7 15.6\"(3...         1,11,030\n",
       "9  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....           85,990"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\") #opening the url\n",
    "time.sleep(10)\n",
    "# code for searching laptop\n",
    "search_input=driver.find_element_by_xpath(\"//div[@class='nav-search-field ']/input\") # xpath for search input\n",
    "search_input.send_keys(\"Laptop\")\n",
    "# Code for button click in search btn\n",
    "search_btn=driver.find_element_by_id(\"nav-search-submit-button\") # xpath for search button\n",
    "search_btn.click()\n",
    "time.sleep(10)\n",
    "# code for intel core i7 fliter \n",
    "try:\n",
    "    filter_i7=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "    for i in filter_i7: # iterate among all filters \n",
    "        if i.text=='Intel Core i7': \n",
    "            i.click()\n",
    "            break\n",
    "except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "    pass\n",
    "\n",
    "# code for intel core i9 fliter \n",
    "try:\n",
    "    filter_i9=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "    for i in filter_i9: # iterate among all filters \n",
    "        if i.text=='Intel Core i9':\n",
    "            i.click()\n",
    "            break\n",
    "except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "    pass\n",
    "# The following code for mouseover on rating and scaping but failed, time constraint also effedted a lot\n",
    "\n",
    "#object of ActionChains\n",
    "#a = ActionChains(driver)\n",
    "#identify element\n",
    "#m = driver.find_element_by_xpath(\"a[@class='a-popover-trigger a-declarative']\")\n",
    "#hover over element\n",
    "#a.move_to_element(m).perform()\n",
    "#identify rating element\n",
    "#n = driver.find_element_xpath(\"span[@class='a-icon-alt']\")\n",
    "\n",
    "time.sleep(5) \n",
    "laptop_title=[]\n",
    "\n",
    "# scraping the titles of laptops\n",
    "title=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")[:10]\n",
    "for i in title:\n",
    "    laptop_title.append(i.text) # saving titles in list\n",
    "\n",
    "laptop_rating=[]\n",
    "\n",
    "# scraping the ratings of laptops\n",
    "\n",
    "rating=driver.find_elements_by_xpath(\"//a[@class='a-popover-trigger a-declarative']/i/span\")[:10]\n",
    "for i in rating:\n",
    "    laptop_rating.append(i.text) # saving ratings of laptop\n",
    "\n",
    "laptop_price=[]\n",
    "\n",
    "# scraping the prices of laptop\n",
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")[:10]\n",
    "for i in price:\n",
    "    laptop_price.append(i.text) # saving prices of laptop\n",
    "time.sleep(5)\n",
    "# checking the equality of list \n",
    "print(len(laptop_title),len(laptop_rating),len(laptop_price))\n",
    "\n",
    "# converting into dataframe\n",
    "df=pd.DataFrame({\"Laptop Title\":laptop_title,\"Rating\":laptop_rating,\"Price\":laptop_price})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc4d59",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "046123e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Post Time</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>10d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steria India Ltd</td>\n",
       "      <td>3d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ameriprise Financial</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jubilant Foodworks Limited</td>\n",
       "      <td>2d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steria India Ltd</td>\n",
       "      <td>22d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Steria India Ltd</td>\n",
       "      <td>22d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HCL Technologies Limited</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zyoin</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GI Group</td>\n",
       "      <td>2d ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Company Name Post Time Rating\n",
       "0  Optum Global Solutions (India) Private Limited   10d ago    4.1\n",
       "1                                Steria India Ltd    3d ago    4.1\n",
       "2                            Ameriprise Financial    5d ago    4.1\n",
       "3                      Jubilant Foodworks Limited    2d ago    4.1\n",
       "4  Optum Global Solutions (India) Private Limited  1mon ago    3.9\n",
       "5                                Steria India Ltd   22d ago    4.1\n",
       "6                                Steria India Ltd   22d ago    4.1\n",
       "7                        HCL Technologies Limited  1mon ago    4.1\n",
       "8                                           Zyoin    4d ago    3.8\n",
       "9                                        GI Group    2d ago    4.2"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.ambitionbox.com/\") #opening the url\n",
    "time.sleep(5)\n",
    "\n",
    "#code for clicking  on job in menu\n",
    "#driver.find_element_by_xpath(\"//div[@class='nav-search-field ']/input\") # xpath for search input\n",
    "\n",
    "job=driver.find_element_by_xpath(\"//nav[@class='left-section']/a[@class='link jobs']\")\n",
    "job.click()\n",
    "\n",
    "# code for enter keyword Data Scientist in Search input\n",
    "search_keys=driver.find_element_by_xpath(\"//input[@title='Enter Designation, Company or a Skill']\")\n",
    "search_keys.send_keys('Data Scientist')\n",
    "\n",
    "# clicking the search button\n",
    "btn=driver.find_element_by_xpath(\"//button[@class='ab_btn search-btn round'][1]/span\")\n",
    "btn.click()\n",
    "time.sleep(2)\n",
    "#clicking the location tab\n",
    "location=driver.find_element_by_xpath('//div[@title=\"Location\"]/p')\n",
    "location.click()\n",
    "time.sleep(2)\n",
    "# code for input Noide in search location\n",
    "search_location=driver.find_element_by_xpath(\"//input[@placeholder='Search locations']\")\n",
    "search_location.send_keys('Noida')\n",
    "time.sleep(2)\n",
    "# code for selecting the Noida location\n",
    "select_noida=driver.find_element_by_id(\"location_Noida\")\n",
    "select_noida.click()\n",
    "time.sleep(2)\n",
    "#code for scraping  the company name\n",
    "company_name=[]\n",
    "name=driver.find_elements_by_xpath(\"//p[@class='company body-medium']\")\n",
    "for i in name:\n",
    "    company_name.append(i.text)\n",
    "#code for scraping  the posting the job\n",
    "post_time=[]\n",
    "post=driver.find_elements_by_xpath(\"//div[@class='other-info']/span[1]\")\n",
    "for i in post:\n",
    "    post_time.append(i.text)\n",
    "#code for scraping the rating of job\n",
    "job_rating=[]\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='rating-wrapper']/a/span\")\n",
    "for i in rating:\n",
    "    job_rating.append(i.text)\n",
    "#checking the length of eash list\n",
    "print(len(company_name),len(post_time),len(job_rating))\n",
    "#converting into dataframe (only 10 Records)\n",
    "df=pd.DataFrame({\"Company Name\":company_name[:10],\"Post Time\":post_time[:10],\"Rating\":job_rating[:10]})\n",
    "driver.close()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e849d15",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6bea50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Total Salaries</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Max Salary</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 10 salaries</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>Data Scientist\\n . \\n4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Optum</td>\n",
       "      <td></td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>Data Scientist\\n . \\n3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 22 salaries</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>Data Scientist\\n . \\n2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td></td>\n",
       "      <td>₹ 8.3L</td>\n",
       "      <td>₹ 8.3L</td>\n",
       "      <td>Data Scientist\\n . \\n3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>based on 60 salaries</td>\n",
       "      <td>₹ 7.2L</td>\n",
       "      <td>₹ 7.2L</td>\n",
       "      <td>Data Scientist\\n . \\n2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Verizon</td>\n",
       "      <td></td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>Data Scientist\\n . \\n4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ganit Business Solutions</td>\n",
       "      <td>based on 24 salaries</td>\n",
       "      <td>₹ 8.5L</td>\n",
       "      <td>₹ 8.5L</td>\n",
       "      <td>Data Scientist\\n . \\n4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deloitte</td>\n",
       "      <td></td>\n",
       "      <td>₹ 6.9L</td>\n",
       "      <td>₹ 6.9L</td>\n",
       "      <td>Data Scientist\\n . \\n2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ernst &amp; Young</td>\n",
       "      <td>based on 45 salaries</td>\n",
       "      <td>₹ 6.0L</td>\n",
       "      <td>₹ 6.0L</td>\n",
       "      <td>Data Scientist\\n . \\n3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td></td>\n",
       "      <td>₹ 5.0L</td>\n",
       "      <td>₹ 5.0L</td>\n",
       "      <td>Data Scientist\\n . \\n3-4 yrs exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Company Name        Total Salaries Minimum Salary Max Salary  \\\n",
       "0                  Ab Inbev  based on 10 salaries        ₹ 15.0L    ₹ 15.0L   \n",
       "1                     Optum                              ₹ 11.0L    ₹ 11.0L   \n",
       "2         Fractal Analytics  based on 22 salaries         ₹ 9.0L     ₹ 9.0L   \n",
       "3           Tiger Analytics                               ₹ 8.3L     ₹ 8.3L   \n",
       "4              UnitedHealth  based on 60 salaries         ₹ 7.2L     ₹ 7.2L   \n",
       "5                   Verizon                              ₹ 10.0L    ₹ 10.0L   \n",
       "6  Ganit Business Solutions  based on 24 salaries         ₹ 8.5L     ₹ 8.5L   \n",
       "7                  Deloitte                               ₹ 6.9L     ₹ 6.9L   \n",
       "8             Ernst & Young  based on 45 salaries         ₹ 6.0L     ₹ 6.0L   \n",
       "9                  Ericsson                               ₹ 5.0L     ₹ 5.0L   \n",
       "\n",
       "                         Experience  \n",
       "0    Data Scientist\\n . \\n4 yrs exp  \n",
       "1  Data Scientist\\n . \\n3-4 yrs exp  \n",
       "2  Data Scientist\\n . \\n2-4 yrs exp  \n",
       "3  Data Scientist\\n . \\n3-4 yrs exp  \n",
       "4  Data Scientist\\n . \\n2-4 yrs exp  \n",
       "5    Data Scientist\\n . \\n4 yrs exp  \n",
       "6    Data Scientist\\n . \\n4 yrs exp  \n",
       "7  Data Scientist\\n . \\n2-4 yrs exp  \n",
       "8  Data Scientist\\n . \\n3-4 yrs exp  \n",
       "9  Data Scientist\\n . \\n3-4 yrs exp  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "driver=webdriver.Firefox(executable_path=\"C:\\\\Users\\\\User\\\\geckodriver.exe\")\n",
    "\n",
    "driver.get(\"https://www.ambitionbox.com/\") #opening the url\n",
    "time.sleep(5)\n",
    "\n",
    "#code for clicking  on job in menu\n",
    "#driver.find_element_by_xpath(\"//div[@class='nav-search-field ']/input\") # xpath for search input\n",
    "\n",
    "salaries=driver.find_element_by_xpath(\"//nav[@class='left-section']/a[@class='link salaries']\")\n",
    "salaries.click()\n",
    "time.sleep(2)\n",
    "job_profile=driver.find_element_by_id(\"jobProfileSearchbox\")\n",
    "job_profile.send_keys(\"Data Scientist\")\n",
    "#suggestion_wrap tt-suggestion tt-selectable\n",
    "time.sleep(5)\n",
    "select=driver.find_elements_by_xpath(\"//div[@class='suggestion_wrap tt-suggestion tt-selectable']\")\n",
    "for i in select:\n",
    "    if(i.text=='Data Scientist'):\n",
    "        i.click()\n",
    "time.sleep(5)\n",
    "# code for scraping company name using xpath\n",
    "company_name=[]\n",
    "name=driver.find_elements_by_xpath(\"//div[@class='company-info']/div/a\")        \n",
    "for i in name:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "time.sleep(2)\n",
    "    \n",
    "# code for scraping total salaries using xpath\n",
    "total_salaries=[]\n",
    "ttl_sal=driver.find_elements_by_xpath(\"//div[@class='company-info']/div/span\")        \n",
    "for i in ttl_sal:\n",
    "    total_salaries.append(i.text.replace('.',''))    \n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "# code for scraping minimum salaries using xpath\n",
    "min_salary=[]\n",
    "min_sal=driver.find_elements_by_xpath(\"//div[@class='salary-values']/div[1]\")        \n",
    "for i in min_sal:\n",
    "    min_salary.append(i.text)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# code for scraping avarage salaries using xpath\n",
    "avg_salary=[]\n",
    "avg_sal=driver.find_elements_by_xpath(\"//div[@class='solid-salary-bar salary-bar-wrapper']/div/p\")  \n",
    "for i in avg_sal:\n",
    "    avg_salary.append(i.text)\n",
    "\n",
    "time.sleep(2)\n",
    "    \n",
    "# code for scraping maximum salaries using xpath\n",
    "max_salary=[]\n",
    "max_sal=driver.find_elements_by_xpath(\"//div[@class='salary-values']/div[2]\")        \n",
    "for i in min_sal:\n",
    "    max_salary.append(i.text)\n",
    "\n",
    "time.sleep(2)    \n",
    "    \n",
    "# code for scraping experience required using xpath\n",
    "experience=[]\n",
    "exp=driver.find_elements_by_xpath(\"//div[@class='salaries sbold-list-header']\")        \n",
    "for i in exp:\n",
    "    experience.append(i.text)\n",
    "\n",
    "time.sleep(2)\n",
    "#checking the eqality of length\n",
    "print(len(company_name),len(total_salaries),len(min_salary),len(max_salary),len(experience))\n",
    "#converting into Dataframe (only 10 Records)\n",
    "df=pd.DataFrame({\"Company Name\":company_name[:10],\"Total Salaries\":total_salaries[:10],\"Minimum Salary\":min_salary[:10],\"Max Salary\":max_salary[:10],\"Experience\":experience[:10]})\n",
    "driver.close()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
